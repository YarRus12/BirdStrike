# BirdStrike
PET project. Pipeline for getting data about aircraft birdstrike incidents

ОГЛАВЛЕНИЕ:
1. Как пользоваться всем этим, если ты просто аналитик и у тебя не времени вникать в вот это вот все
2. Структура проекта
3. Структура Базы данных
4. Работа DAG
5. Особенности работы с данными


Как пользоваться всем этим, если ты просто аналитик и у тебя не времени вникать в вот это вот все

Скачиваешь репозиторий себе, лучше с помощью IDE или VS Code
Убеждаешься, что у тебя есть возможность запускать docker
Заходишь в папку с файлом docker-compose.yaml через терминал или любой другой CLI
Вводишь команду docker-compose up ~~молишься чтобы заработало~~
Подключаешься к контейнеру с POSTGRESQL под профилем airflow, пароль airflow, база данных airflow
host localhost, port 6432 через любимый dbeaver или, что душе угодно
![img_1.png](media/img_1.png)
Работаешь работу.

Данные по инцидентам и данные о погоде во время инцидента на ближайщей станции наблюдения находятся в таблицах DDS.aircraft_incidents и DDS.weather_observation
в базу автоматически загружены данные с 1 января 2018 по 31 декабря 2020 года в момент создания контейнера
Для запуска обновления через airflow в файле main нужно внести изменния в конфигурацию конкретной таски. 
А именно, нужно изменить даты загрузок
Для загрузки данных о погоде

    task_weather_data = PythonOperator(
        task_id='download_weather_data',
        python_callable=stg_loadings.weather_data,
        op_kwargs={'controller': stg_loadings,
                   'start_date': datetime.datetime(year=2020, month=1, day=1), -- ТУТА!
                   'end_date': datetime.datetime(year=2021, month=12, day=31)}) -- ТУТА!!

Для загрузки данных об инцидентах можно ничего не менять. 
Каждый запуск данные будут подгружаться за каждые 4 недели пока не достигнут текущего дня

    task_animal_incidents = PythonOperator(
        task_id='download_animal_incidents',
        python_callable=stg_loadings.animal_incidents_data)

И конечно запустить сам даг  
![img_5.png](media/img_5.png)

Не понятно или не работает, пиши в телеге @YarRuss12


** Структура проекта **  
|-- docker-compose.yaml - файл с инструкцией по созданию докер-контейнеров для приложения
|-- .env - файл с переменными окружения для контейнеров airflow (пока просто на перспективу)
|-- Dockerfile - файл с детализацией инструкций по созданию контейнеров airflow
|-- requirements.txt - сторонние библиотеки (осторожно, возможно в нем много лишних!!!)
|-- Database
    |-- Dockerfile - файл с детализацией инструкций по созданию контейнера для базы данных
    |-- 0_init_db.sql - инструкция сборки базы данных для инцидентов и погоды
    |-- 1_aircraft_incidents.sql - dump с таблицей об инцидентах детализированного слоя
    |-- 2_weather_observation.sql - dump с таблицей о погоде приминительно к каждому инциденту
|-- dags - папка с ДАГами и модулями
    |-- main.py - основной ДАГ с загрузкой полного цикла
    |-- only_incidents.py - ДАГ, загружающий сведения только об инцидентах
    |-- only_weather.py - ДАГ, загружающий сведения только о погоде принимительно к конкретному инциденту
    |-- config.py - модуль с параметрами подключения к базам данных (сейчас только 1)
    |-- modules
       |-- str_loader.py -- модуль с классом StgControler для работы с сырыми данными
       |-- dds_loader.py -- модуль с классом DdsControler для загрузки данных в детализированный слой
       |-- cdm_loader.py -- модуль с классом CdmControler для формирования datamart и выгрузки csv файлов
       |-- instruments.py -- модуль с прикладными функциями(некоторые не пригодились)
       |-- connections.py -- модуль с классом для подключения к базам данных


** Структура Базы данных **
    Схема Stage
        aircraft_incidents -- сырые данные об инцидентах
        observation_reference -- справочник о станциях наблюдения за погодой
        weather_observation -- сырые данные о погоде
    Схема DDS
        aircraft_incidents -- обработанные данные об инцидентах
        observation_reference -- актуальный справочник станций наблюдений за погодой
        incident_station_link -- таблица соединяющая инцидент с ближайшей станцией
        weather_observation -- таблица с данными о погоде для конкретного инцидента
    Схема CDM
        top_ten_airports -- таблица с топ 10 аэропортах по числу инцидентов
        airport_bts_name -- таблица-справочник соединяющая id аэропорта с наименованиями, принятыми BTS

** Запуск проекта **
Файл docker-compose.yaml с помощью команды docker-compose up -d создает докер-контейнеры с СУБД POSTGRESQL, airflow и remote webserver
В контейнере birdstrike-database-1 создается требуемая базу данных, из дампов формируются таблицы dds.aircraft_incidents и dds.weather_observation
Создаются папки Archives, Unresolved, Downloads
Airflow и remote webdriver работают через передачу скаченных файлов из volume папки Downloads


Работа DAG
Опишим работу общего ДАГа main.py (два других это просто усеченные версии)
![img.png](media/img.png)

1. Загрузка справочника о станциях
   Важно! В National Centers for Environmental Information формат id станции это 11 символов, USAF и WBAN могут быть менее заданного числа
   Поэтому перед соединением следуем добавить к USAF неоходимое число нулей, чтобы длинна USAF была 6 символов
   Для WBAN требуется такая же процедура, но число символов должно быть 5
2. Обновление справочника в DDS слое
3. Выполняется загрузка данных о авиационных инцидентах с участием животных
   На заданные пользователем даты отбирается выборка из базы данных Federal Aviation Administration через remote webdriver. 
   Чем больше выборка, тем больше времени нужно чтобы сайт собрал excel файл и тем больше времени нужно selenium webdriver чтобы скачать файл.
        Поэтому до перехода к следующему этапу выполняется time.sleep(n), и несколько попыток скачать собранный файл
   Файл сохраняется remote webdriver в папку volumes Downloads, откуда его берет для обработки airflow
   Полученный файл разархивируется и сохраняется в dataframe, после чего заливается в базу данных
4. Сырые данные об инцидентах обрабатываются и переливаются в DDS слой
5. На данных об инцидентах и справочных данных о станциях наблюдений создается таблица связей между инцидентом и станцией
6. Из таблицы инцидентов создается упорядоченная выборка за период.
   Из выборки отбирается диапазон данных и станции. Эти сведения нужны для формирования запроса на получение csv файла с сайта National Centers for Environmental Information
   Запрос формируется не из всей выборки, а из небольших батчей по 50 записей, так как единовременно можно послать запрос на сайт только из 50 записей
   Полученный файл сохраняется remote webdriver в папку volumes Downloads, откуда его берет для обработки airflow
   Файл преобразуется в датафрейм и заливается в слой сырых данных.
7. Сведения о погоде, через таблицу связей объединяются с инцидентом и заливаются в таблицу dds.weather_observation
8. Загрузка данных о 10 аэропортах с наибольшим числом инцидетов с животными
        Из таблицы DDS.aircraft_incidents выбираются 11 аэропортоа с наибольшим числом инцидетов, Через inner join выполняется соединение выборки с табилцей DDS.airport_bts_name.  
        DDS.airport_bts_name пока создается вручную. Это справочник с названиями аэропортов на сайте Bureau of Transport Statistics
        В pезультате Inner join происходит фильтрация аэропортов, не указанных в справочнике. Это плохо, но сейчас это удаляет из выборки 11 аэропорт с названием "UNKNOWN". С другой стороны обработать UNKNOWN аэропорт на сайте BTS не представляется возможным.
        Созданный список tuples_airports из id, названия аэропорта по система Federal Aviation Administration и названия аэроторта по системе Bureau of Transport Statistics передается методу "top_airports_traffic"
        С помощью Selenium webdriver открывается страница Bureau of Transport Statistics, выбирается вкладка 'Link_Flights'. Каждый аэропорт в tuples_airports обрабатывается, вызывается таблица со вседениями об отбывающих рейсах и о прибывающих рейсах.
        Полученные записи записываются в cdm.top_ten_airports с указанием id, названия, статистики и даты построения выборки.

















